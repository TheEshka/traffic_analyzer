{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"./dimapac.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ip_from_string(ips):\n",
    "    return \"\".join(chr(int(n)) for n in ips.split(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deopped = df.drop(['route', 'timestamp'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4687, 27)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deopped.fillna(0)\n",
    "# df_deopped = df_deopped.reset_index()\n",
    "clf = OneClassSVM(kernel='linear', gamma=0.001, nu=0.95).fit(df_deopped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = clf.predict(df_deopped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([-1, 1])\n",
      "dict_values([4452, 235])\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(y).keys()) # equals to list(set(words))\n",
    "print(Counter(y).values()) # counts the elements' frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05013868145935566"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "235/(4452+235)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4687/4687 [==============================] - 1s 142us/sample - loss: nan\n",
      "Epoch 2/100\n",
      "4687/4687 [==============================] - 0s 61us/sample - loss: nan\n",
      "Epoch 3/100\n",
      "4687/4687 [==============================] - 0s 57us/sample - loss: nan\n",
      "Epoch 4/100\n",
      "4687/4687 [==============================] - 0s 59us/sample - loss: nan\n",
      "Epoch 5/100\n",
      "4687/4687 [==============================] - 0s 59us/sample - loss: nan\n",
      "Epoch 6/100\n",
      "4687/4687 [==============================] - 0s 56us/sample - loss: nan\n",
      "Epoch 7/100\n",
      "4687/4687 [==============================] - 0s 55us/sample - loss: nan\n",
      "Epoch 8/100\n",
      "4687/4687 [==============================] - 0s 55us/sample - loss: nan\n",
      "Epoch 9/100\n",
      "4687/4687 [==============================] - 0s 54us/sample - loss: nan\n",
      "Epoch 10/100\n",
      "4687/4687 [==============================] - 0s 55us/sample - loss: nan\n",
      "Epoch 11/100\n",
      "4687/4687 [==============================] - 0s 54us/sample - loss: nan\n",
      "Epoch 12/100\n",
      "4687/4687 [==============================] - 0s 54us/sample - loss: nan\n",
      "Epoch 13/100\n",
      "4687/4687 [==============================] - 0s 53us/sample - loss: nan\n",
      "Epoch 14/100\n",
      "4687/4687 [==============================] - 0s 55us/sample - loss: nan\n",
      "Epoch 15/100\n",
      "4687/4687 [==============================] - 0s 54us/sample - loss: nan\n",
      "Epoch 16/100\n",
      "4687/4687 [==============================] - 0s 54us/sample - loss: nan\n",
      "Epoch 17/100\n",
      "4687/4687 [==============================] - 0s 54us/sample - loss: nan\n",
      "Epoch 18/100\n",
      "4687/4687 [==============================] - 0s 57us/sample - loss: nan\n",
      "Epoch 19/100\n",
      "4687/4687 [==============================] - 0s 58us/sample - loss: nan\n",
      "Epoch 20/100\n",
      "4687/4687 [==============================] - 0s 57us/sample - loss: nan\n",
      "Epoch 21/100\n",
      "4687/4687 [==============================] - 0s 60us/sample - loss: nan\n",
      "Epoch 22/100\n",
      "4687/4687 [==============================] - 0s 71us/sample - loss: nan\n",
      "Epoch 23/100\n",
      "4687/4687 [==============================] - 0s 56us/sample - loss: nan\n",
      "Epoch 24/100\n",
      "4687/4687 [==============================] - 0s 56us/sample - loss: nan\n",
      "Epoch 25/100\n",
      "4687/4687 [==============================] - 0s 54us/sample - loss: nan\n",
      "Epoch 26/100\n",
      "4687/4687 [==============================] - 0s 60us/sample - loss: nan\n",
      "Epoch 27/100\n",
      "4687/4687 [==============================] - 0s 61us/sample - loss: nan\n",
      "Epoch 28/100\n",
      "4687/4687 [==============================] - 0s 62us/sample - loss: nan\n",
      "Epoch 29/100\n",
      "4687/4687 [==============================] - 0s 70us/sample - loss: nan\n",
      "Epoch 30/100\n",
      "4687/4687 [==============================] - 0s 56us/sample - loss: nan\n",
      "Epoch 31/100\n",
      "4687/4687 [==============================] - 0s 62us/sample - loss: nan\n",
      "Epoch 32/100\n",
      "4687/4687 [==============================] - 0s 56us/sample - loss: nan\n",
      "Epoch 33/100\n",
      "4687/4687 [==============================] - 0s 62us/sample - loss: nan\n",
      "Epoch 34/100\n",
      "4687/4687 [==============================] - 0s 54us/sample - loss: nan\n",
      "Epoch 35/100\n",
      "4687/4687 [==============================] - 0s 54us/sample - loss: nan\n",
      "Epoch 36/100\n",
      "4687/4687 [==============================] - 0s 60us/sample - loss: nan\n",
      "Epoch 37/100\n",
      "4687/4687 [==============================] - 0s 56us/sample - loss: nan\n",
      "Epoch 38/100\n",
      "4687/4687 [==============================] - 0s 55us/sample - loss: nan\n",
      "Epoch 39/100\n",
      "4687/4687 [==============================] - 0s 57us/sample - loss: nan\n",
      "Epoch 40/100\n",
      "4687/4687 [==============================] - 0s 87us/sample - loss: nan\n",
      "Epoch 41/100\n",
      "4687/4687 [==============================] - 0s 51us/sample - loss: nan\n",
      "Epoch 42/100\n",
      "4687/4687 [==============================] - 0s 51us/sample - loss: nan\n",
      "Epoch 43/100\n",
      "4687/4687 [==============================] - 0s 57us/sample - loss: nan\n",
      "Epoch 44/100\n",
      "4687/4687 [==============================] - 0s 51us/sample - loss: nan\n",
      "Epoch 45/100\n",
      "4687/4687 [==============================] - 0s 51us/sample - loss: nan\n",
      "Epoch 46/100\n",
      "4687/4687 [==============================] - 0s 51us/sample - loss: nan\n",
      "Epoch 47/100\n",
      "4687/4687 [==============================] - 0s 55us/sample - loss: nan\n",
      "Epoch 48/100\n",
      "4687/4687 [==============================] - 0s 86us/sample - loss: nan\n",
      "Epoch 49/100\n",
      "4687/4687 [==============================] - 0s 98us/sample - loss: nan\n",
      "Epoch 50/100\n",
      "4687/4687 [==============================] - 1s 133us/sample - loss: nan\n",
      "Epoch 51/100\n",
      "4687/4687 [==============================] - 0s 84us/sample - loss: nan\n",
      "Epoch 52/100\n",
      "4687/4687 [==============================] - 0s 67us/sample - loss: nan\n",
      "Epoch 53/100\n",
      "4687/4687 [==============================] - 0s 64us/sample - loss: nan\n",
      "Epoch 54/100\n",
      "4687/4687 [==============================] - 0s 78us/sample - loss: nan\n",
      "Epoch 55/100\n",
      "4687/4687 [==============================] - 0s 58us/sample - loss: nan\n",
      "Epoch 56/100\n",
      "4687/4687 [==============================] - 0s 56us/sample - loss: nan\n",
      "Epoch 57/100\n",
      "4687/4687 [==============================] - 0s 57us/sample - loss: nan\n",
      "Epoch 58/100\n",
      "4687/4687 [==============================] - 0s 63us/sample - loss: nan\n",
      "Epoch 59/100\n",
      "4687/4687 [==============================] - 0s 52us/sample - loss: nan\n",
      "Epoch 60/100\n",
      "4687/4687 [==============================] - 0s 53us/sample - loss: nan\n",
      "Epoch 61/100\n",
      "4687/4687 [==============================] - 0s 56us/sample - loss: nan\n",
      "Epoch 62/100\n",
      "4687/4687 [==============================] - 0s 52us/sample - loss: nan\n",
      "Epoch 63/100\n",
      "4687/4687 [==============================] - 0s 58us/sample - loss: nan\n",
      "Epoch 64/100\n",
      "4687/4687 [==============================] - 0s 56us/sample - loss: nan\n",
      "Epoch 65/100\n",
      "4687/4687 [==============================] - 0s 53us/sample - loss: nan\n",
      "Epoch 66/100\n",
      "4687/4687 [==============================] - 0s 54us/sample - loss: nan\n",
      "Epoch 67/100\n",
      "4687/4687 [==============================] - 0s 53us/sample - loss: nan\n",
      "Epoch 68/100\n",
      "4687/4687 [==============================] - 0s 51us/sample - loss: nan\n",
      "Epoch 69/100\n",
      "4687/4687 [==============================] - 0s 53us/sample - loss: nan\n",
      "Epoch 70/100\n",
      "4687/4687 [==============================] - 0s 55us/sample - loss: nan\n",
      "Epoch 71/100\n",
      "4687/4687 [==============================] - 0s 53us/sample - loss: nan\n",
      "Epoch 72/100\n",
      "4687/4687 [==============================] - 0s 57us/sample - loss: nan\n",
      "Epoch 73/100\n",
      "4687/4687 [==============================] - 0s 54us/sample - loss: nan\n",
      "Epoch 74/100\n",
      "4687/4687 [==============================] - 0s 52us/sample - loss: nan\n",
      "Epoch 75/100\n",
      "4687/4687 [==============================] - 0s 51us/sample - loss: nan\n",
      "Epoch 76/100\n",
      "4687/4687 [==============================] - 0s 55us/sample - loss: nan\n",
      "Epoch 77/100\n",
      "4687/4687 [==============================] - 0s 54us/sample - loss: nan\n",
      "Epoch 78/100\n",
      "4687/4687 [==============================] - 0s 53us/sample - loss: nan\n",
      "Epoch 79/100\n",
      "4687/4687 [==============================] - 0s 52us/sample - loss: nan\n",
      "Epoch 80/100\n",
      "4687/4687 [==============================] - 0s 56us/sample - loss: nan\n",
      "Epoch 81/100\n",
      "4687/4687 [==============================] - 0s 60us/sample - loss: nan\n",
      "Epoch 82/100\n",
      "4687/4687 [==============================] - 0s 55us/sample - loss: nan\n",
      "Epoch 83/100\n",
      "4687/4687 [==============================] - 0s 57us/sample - loss: nan\n",
      "Epoch 84/100\n",
      "4687/4687 [==============================] - 0s 56us/sample - loss: nan\n",
      "Epoch 85/100\n",
      "4687/4687 [==============================] - 0s 55us/sample - loss: nan\n",
      "Epoch 86/100\n",
      "4687/4687 [==============================] - 0s 52us/sample - loss: nan\n",
      "Epoch 87/100\n",
      "4687/4687 [==============================] - 0s 69us/sample - loss: nan\n",
      "Epoch 88/100\n",
      "4687/4687 [==============================] - 0s 67us/sample - loss: nan\n",
      "Epoch 89/100\n",
      "4687/4687 [==============================] - 0s 58us/sample - loss: nan\n",
      "Epoch 90/100\n",
      "4687/4687 [==============================] - 0s 55us/sample - loss: nan\n",
      "Epoch 91/100\n",
      "4687/4687 [==============================] - 0s 56us/sample - loss: nan\n",
      "Epoch 92/100\n",
      "4687/4687 [==============================] - 0s 58us/sample - loss: nan\n",
      "Epoch 93/100\n",
      "4687/4687 [==============================] - 0s 52us/sample - loss: nan\n",
      "Epoch 94/100\n",
      "4687/4687 [==============================] - 0s 52us/sample - loss: nan\n",
      "Epoch 95/100\n",
      "4687/4687 [==============================] - 0s 57us/sample - loss: nan\n",
      "Epoch 96/100\n",
      "4687/4687 [==============================] - 0s 54us/sample - loss: nan\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4687/4687 [==============================] - 0s 51us/sample - loss: nan\n",
      "Epoch 98/100\n",
      "4687/4687 [==============================] - 0s 51us/sample - loss: nan\n",
      "Epoch 99/100\n",
      "4687/4687 [==============================] - 0s 56us/sample - loss: nan\n",
      "Epoch 100/100\n",
      "4687/4687 [==============================] - 0s 54us/sample - loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f870c58dbe0>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=df_deopped.shape[1], activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(df_deopped.shape[1])) # Multiple output neurons\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(df_deopped,df_deopped,verbose=1,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-a13d3070545e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_deopped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscore1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_deopped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# pred = model.predict(x_normal)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# score2 = np.sqrt(metrics.mean_squared_error(pred,x_normal))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# pred = model.predict(x_attack)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \"\"\"\n\u001b[1;32m    253\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 254\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    255\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[1;32m     84\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 646\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                     (type_err,\n\u001b[0;32m--> 100\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m    101\u001b[0m             )\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "pred = model.predict(df_deopped)\n",
    "score1 = np.sqrt(metrics.mean_squared_error(pred,df_deopped))\n",
    "# pred = model.predict(x_normal)\n",
    "# score2 = np.sqrt(metrics.mean_squared_error(pred,x_normal))\n",
    "# pred = model.predict(x_attack)\n",
    "# score3 = np.sqrt(metrics.mean_squared_error(pred,x_attack))\n",
    "# print(f\"Out of Sample Score (RMSE): {score1}\")\n",
    "# print(f\"Insample Normal Score (RMSE): {score2}\")\n",
    "# print(f\"Attack Underway Score (RMSE): {score3}\")\n",
    "score1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create neural net\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=encdf.shape[1], activation='relu'))\n",
    "model.add(Dense(50, input_dim=encdf.shape[1], activation='relu'))\n",
    "model.add(Dense(10, input_dim=encdf.shape[1], activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
    "                        patience=5, verbose=1, mode='auto')\n",
    "model.fit(encdf,encdf,validation_data=(x_test,y_test),\n",
    "          callbacks=[monitor],verbose=2,epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "encdf= df_deopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_numeric_zscore(encdf, 'client_package_size_mean')\n",
    "encode_numeric_zscore(encdf, 'client_package_size_std') #2\n",
    "encode_numeric_zscore(encdf, 'server_package_size_mean') #3\n",
    "encode_numeric_zscore(encdf, 'server_package_size_std') #4\n",
    "encode_numeric_zscore(encdf, 'client_batch_sizes_mean') #5\n",
    "encode_numeric_zscore(encdf, 'client_batch_sizes_std') #6\n",
    "encode_numeric_zscore(encdf, 'server_batch_sizes_mean') #7\n",
    "encode_numeric_zscore(encdf, 'server_batch_sizes_std') #8\n",
    "encode_numeric_zscore(encdf, 'client_batch_counts_mean') #9\n",
    "encode_numeric_zscore(encdf, 'server_batch_counts_mean') #10\n",
    "encode_numeric_zscore(encdf, 'client_efficiency') #11\n",
    "encode_numeric_zscore(encdf, 'server_efficiency') #12\n",
    "encode_numeric_zscore(encdf, 'ratio_sizes') #13\n",
    "encode_numeric_zscore(encdf, 'ratio_application_size') #14\n",
    "encode_numeric_zscore(encdf, 'ratio_packages') #15\n",
    "encode_numeric_zscore(encdf, 'client_package_size_sum') #16\n",
    "encode_numeric_zscore(encdf, 'client_application_size_sum')#17\n",
    "encode_numeric_zscore(encdf, 'client_package_count') #18\n",
    "encode_numeric_zscore(encdf, 'client_batch_counts_sum') #19\n",
    "encode_numeric_zscore(encdf, 'server_package_size_sum') #20\n",
    "encode_numeric_zscore(encdf, 'server_application_size_sum') #21\n",
    "encode_numeric_zscore(encdf, 'server_package_count')#22\n",
    "encode_numeric_zscore(encdf, 'server_batch_counts_sum') #23\n",
    "encode_numeric_zscore(encdf, 'transport_protocol') #24\n",
    "encode_numeric_zscore(encdf, 'ip_protocol_version') #25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_package_size_mean</th>\n",
       "      <th>client_package_size_std</th>\n",
       "      <th>server_package_size_mean</th>\n",
       "      <th>server_package_size_std</th>\n",
       "      <th>client_batch_sizes_mean</th>\n",
       "      <th>client_batch_sizes_std</th>\n",
       "      <th>server_batch_sizes_mean</th>\n",
       "      <th>server_batch_sizes_std</th>\n",
       "      <th>client_batch_counts_mean</th>\n",
       "      <th>server_batch_counts_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>client_package_size_sum</th>\n",
       "      <th>client_application_size_sum</th>\n",
       "      <th>client_package_count</th>\n",
       "      <th>client_batch_counts_sum</th>\n",
       "      <th>server_package_size_sum</th>\n",
       "      <th>server_application_size_sum</th>\n",
       "      <th>server_package_count</th>\n",
       "      <th>server_batch_counts_sum</th>\n",
       "      <th>transport_protocol</th>\n",
       "      <th>ip_protocol_version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.016010</td>\n",
       "      <td>0.139302</td>\n",
       "      <td>0.534552</td>\n",
       "      <td>1.332992</td>\n",
       "      <td>-0.100922</td>\n",
       "      <td>-0.102119</td>\n",
       "      <td>-0.093141</td>\n",
       "      <td>-0.125591</td>\n",
       "      <td>-0.104925</td>\n",
       "      <td>-0.171883</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.289119</td>\n",
       "      <td>-0.279058</td>\n",
       "      <td>-0.261347</td>\n",
       "      <td>-0.222037</td>\n",
       "      <td>-0.198996</td>\n",
       "      <td>-0.192753</td>\n",
       "      <td>-0.240515</td>\n",
       "      <td>-0.221811</td>\n",
       "      <td>-0.270214</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.381205</td>\n",
       "      <td>-0.440794</td>\n",
       "      <td>0.546090</td>\n",
       "      <td>1.408523</td>\n",
       "      <td>-0.137482</td>\n",
       "      <td>-0.107879</td>\n",
       "      <td>-0.039763</td>\n",
       "      <td>-0.145785</td>\n",
       "      <td>-0.271101</td>\n",
       "      <td>-0.088165</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.296871</td>\n",
       "      <td>-0.289884</td>\n",
       "      <td>-0.261347</td>\n",
       "      <td>-0.228781</td>\n",
       "      <td>-0.200888</td>\n",
       "      <td>-0.194786</td>\n",
       "      <td>-0.242920</td>\n",
       "      <td>-0.228555</td>\n",
       "      <td>-0.270214</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.479361</td>\n",
       "      <td>1.911596</td>\n",
       "      <td>0.311961</td>\n",
       "      <td>1.070377</td>\n",
       "      <td>0.098181</td>\n",
       "      <td>0.077524</td>\n",
       "      <td>-0.080703</td>\n",
       "      <td>-0.099582</td>\n",
       "      <td>0.061252</td>\n",
       "      <td>-0.125373</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.236032</td>\n",
       "      <td>-0.224063</td>\n",
       "      <td>-0.230890</td>\n",
       "      <td>-0.215293</td>\n",
       "      <td>-0.188668</td>\n",
       "      <td>-0.183200</td>\n",
       "      <td>-0.223680</td>\n",
       "      <td>-0.215067</td>\n",
       "      <td>-0.270214</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.547619</td>\n",
       "      <td>-1.044980</td>\n",
       "      <td>-1.051694</td>\n",
       "      <td>-1.318745</td>\n",
       "      <td>-0.175741</td>\n",
       "      <td>-0.107879</td>\n",
       "      <td>-0.161106</td>\n",
       "      <td>-0.145785</td>\n",
       "      <td>0.061252</td>\n",
       "      <td>-0.143977</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.301547</td>\n",
       "      <td>-0.294189</td>\n",
       "      <td>-0.264393</td>\n",
       "      <td>-0.228781</td>\n",
       "      <td>-0.211496</td>\n",
       "      <td>-0.205731</td>\n",
       "      <td>-0.242920</td>\n",
       "      <td>-0.228555</td>\n",
       "      <td>-0.270214</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.003113</td>\n",
       "      <td>1.539662</td>\n",
       "      <td>-0.004350</td>\n",
       "      <td>0.329613</td>\n",
       "      <td>-0.007268</td>\n",
       "      <td>0.036231</td>\n",
       "      <td>-0.138624</td>\n",
       "      <td>-0.109939</td>\n",
       "      <td>0.194193</td>\n",
       "      <td>-0.153279</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.238258</td>\n",
       "      <td>-0.219062</td>\n",
       "      <td>-0.246119</td>\n",
       "      <td>-0.201804</td>\n",
       "      <td>-0.197683</td>\n",
       "      <td>-0.191620</td>\n",
       "      <td>-0.230895</td>\n",
       "      <td>-0.194833</td>\n",
       "      <td>3.699982</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11026</th>\n",
       "      <td>0.890810</td>\n",
       "      <td>2.262547</td>\n",
       "      <td>-0.073772</td>\n",
       "      <td>0.895985</td>\n",
       "      <td>0.165794</td>\n",
       "      <td>0.141471</td>\n",
       "      <td>-0.105339</td>\n",
       "      <td>-0.066673</td>\n",
       "      <td>0.227428</td>\n",
       "      <td>-0.088165</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.143257</td>\n",
       "      <td>-0.122923</td>\n",
       "      <td>-0.191297</td>\n",
       "      <td>-0.195060</td>\n",
       "      <td>-0.184283</td>\n",
       "      <td>-0.180071</td>\n",
       "      <td>-0.204439</td>\n",
       "      <td>-0.201578</td>\n",
       "      <td>-0.270214</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11027</th>\n",
       "      <td>0.309429</td>\n",
       "      <td>0.867134</td>\n",
       "      <td>-0.505882</td>\n",
       "      <td>-0.517146</td>\n",
       "      <td>-0.031762</td>\n",
       "      <td>-0.087879</td>\n",
       "      <td>-0.143160</td>\n",
       "      <td>-0.135319</td>\n",
       "      <td>-0.104925</td>\n",
       "      <td>-0.171883</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.278469</td>\n",
       "      <td>-0.268524</td>\n",
       "      <td>-0.258301</td>\n",
       "      <td>-0.222037</td>\n",
       "      <td>-0.207872</td>\n",
       "      <td>-0.202118</td>\n",
       "      <td>-0.242920</td>\n",
       "      <td>-0.221811</td>\n",
       "      <td>-0.270214</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11028</th>\n",
       "      <td>-0.002797</td>\n",
       "      <td>0.992521</td>\n",
       "      <td>0.331343</td>\n",
       "      <td>0.874635</td>\n",
       "      <td>-0.016444</td>\n",
       "      <td>-0.014364</td>\n",
       "      <td>-0.085676</td>\n",
       "      <td>-0.086748</td>\n",
       "      <td>0.282821</td>\n",
       "      <td>-0.106769</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258094</td>\n",
       "      <td>-0.255627</td>\n",
       "      <td>-0.227845</td>\n",
       "      <td>-0.215293</td>\n",
       "      <td>-0.190111</td>\n",
       "      <td>-0.185090</td>\n",
       "      <td>-0.226085</td>\n",
       "      <td>-0.215067</td>\n",
       "      <td>-0.270214</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11029</th>\n",
       "      <td>-0.412289</td>\n",
       "      <td>-0.680275</td>\n",
       "      <td>-0.707775</td>\n",
       "      <td>-0.805078</td>\n",
       "      <td>-0.182349</td>\n",
       "      <td>-0.096700</td>\n",
       "      <td>-0.154367</td>\n",
       "      <td>-0.138143</td>\n",
       "      <td>-0.250329</td>\n",
       "      <td>-0.189324</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246279</td>\n",
       "      <td>-0.267297</td>\n",
       "      <td>-0.160840</td>\n",
       "      <td>-0.127617</td>\n",
       "      <td>-0.190816</td>\n",
       "      <td>-0.191194</td>\n",
       "      <td>-0.168363</td>\n",
       "      <td>-0.127389</td>\n",
       "      <td>-0.270214</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11030</th>\n",
       "      <td>3.742169</td>\n",
       "      <td>3.298328</td>\n",
       "      <td>-0.927935</td>\n",
       "      <td>-1.156163</td>\n",
       "      <td>2.507224</td>\n",
       "      <td>1.079296</td>\n",
       "      <td>-0.152798</td>\n",
       "      <td>-0.141783</td>\n",
       "      <td>2.387723</td>\n",
       "      <td>-0.116071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059545</td>\n",
       "      <td>0.124259</td>\n",
       "      <td>-0.203479</td>\n",
       "      <td>-0.222037</td>\n",
       "      <td>-0.208925</td>\n",
       "      <td>-0.204349</td>\n",
       "      <td>-0.230895</td>\n",
       "      <td>-0.221811</td>\n",
       "      <td>-0.270214</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4687 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       client_package_size_mean  client_package_size_std  \\\n",
       "0                     -0.016010                 0.139302   \n",
       "1                     -0.381205                -0.440794   \n",
       "8                      0.479361                 1.911596   \n",
       "10                    -0.547619                -1.044980   \n",
       "19                     1.003113                 1.539662   \n",
       "...                         ...                      ...   \n",
       "11026                  0.890810                 2.262547   \n",
       "11027                  0.309429                 0.867134   \n",
       "11028                 -0.002797                 0.992521   \n",
       "11029                 -0.412289                -0.680275   \n",
       "11030                  3.742169                 3.298328   \n",
       "\n",
       "       server_package_size_mean  server_package_size_std  \\\n",
       "0                      0.534552                 1.332992   \n",
       "1                      0.546090                 1.408523   \n",
       "8                      0.311961                 1.070377   \n",
       "10                    -1.051694                -1.318745   \n",
       "19                    -0.004350                 0.329613   \n",
       "...                         ...                      ...   \n",
       "11026                 -0.073772                 0.895985   \n",
       "11027                 -0.505882                -0.517146   \n",
       "11028                  0.331343                 0.874635   \n",
       "11029                 -0.707775                -0.805078   \n",
       "11030                 -0.927935                -1.156163   \n",
       "\n",
       "       client_batch_sizes_mean  client_batch_sizes_std  \\\n",
       "0                    -0.100922               -0.102119   \n",
       "1                    -0.137482               -0.107879   \n",
       "8                     0.098181                0.077524   \n",
       "10                   -0.175741               -0.107879   \n",
       "19                   -0.007268                0.036231   \n",
       "...                        ...                     ...   \n",
       "11026                 0.165794                0.141471   \n",
       "11027                -0.031762               -0.087879   \n",
       "11028                -0.016444               -0.014364   \n",
       "11029                -0.182349               -0.096700   \n",
       "11030                 2.507224                1.079296   \n",
       "\n",
       "       server_batch_sizes_mean  server_batch_sizes_std  \\\n",
       "0                    -0.093141               -0.125591   \n",
       "1                    -0.039763               -0.145785   \n",
       "8                    -0.080703               -0.099582   \n",
       "10                   -0.161106               -0.145785   \n",
       "19                   -0.138624               -0.109939   \n",
       "...                        ...                     ...   \n",
       "11026                -0.105339               -0.066673   \n",
       "11027                -0.143160               -0.135319   \n",
       "11028                -0.085676               -0.086748   \n",
       "11029                -0.154367               -0.138143   \n",
       "11030                -0.152798               -0.141783   \n",
       "\n",
       "       client_batch_counts_mean  server_batch_counts_mean  ...  \\\n",
       "0                     -0.104925                 -0.171883  ...   \n",
       "1                     -0.271101                 -0.088165  ...   \n",
       "8                      0.061252                 -0.125373  ...   \n",
       "10                     0.061252                 -0.143977  ...   \n",
       "19                     0.194193                 -0.153279  ...   \n",
       "...                         ...                       ...  ...   \n",
       "11026                  0.227428                 -0.088165  ...   \n",
       "11027                 -0.104925                 -0.171883  ...   \n",
       "11028                  0.282821                 -0.106769  ...   \n",
       "11029                 -0.250329                 -0.189324  ...   \n",
       "11030                  2.387723                 -0.116071  ...   \n",
       "\n",
       "       client_package_size_sum  client_application_size_sum  \\\n",
       "0                    -0.289119                    -0.279058   \n",
       "1                    -0.296871                    -0.289884   \n",
       "8                    -0.236032                    -0.224063   \n",
       "10                   -0.301547                    -0.294189   \n",
       "19                   -0.238258                    -0.219062   \n",
       "...                        ...                          ...   \n",
       "11026                -0.143257                    -0.122923   \n",
       "11027                -0.278469                    -0.268524   \n",
       "11028                -0.258094                    -0.255627   \n",
       "11029                -0.246279                    -0.267297   \n",
       "11030                 0.059545                     0.124259   \n",
       "\n",
       "       client_package_count  client_batch_counts_sum  server_package_size_sum  \\\n",
       "0                 -0.261347                -0.222037                -0.198996   \n",
       "1                 -0.261347                -0.228781                -0.200888   \n",
       "8                 -0.230890                -0.215293                -0.188668   \n",
       "10                -0.264393                -0.228781                -0.211496   \n",
       "19                -0.246119                -0.201804                -0.197683   \n",
       "...                     ...                      ...                      ...   \n",
       "11026             -0.191297                -0.195060                -0.184283   \n",
       "11027             -0.258301                -0.222037                -0.207872   \n",
       "11028             -0.227845                -0.215293                -0.190111   \n",
       "11029             -0.160840                -0.127617                -0.190816   \n",
       "11030             -0.203479                -0.222037                -0.208925   \n",
       "\n",
       "       server_application_size_sum  server_package_count  \\\n",
       "0                        -0.192753             -0.240515   \n",
       "1                        -0.194786             -0.242920   \n",
       "8                        -0.183200             -0.223680   \n",
       "10                       -0.205731             -0.242920   \n",
       "19                       -0.191620             -0.230895   \n",
       "...                            ...                   ...   \n",
       "11026                    -0.180071             -0.204439   \n",
       "11027                    -0.202118             -0.242920   \n",
       "11028                    -0.185090             -0.226085   \n",
       "11029                    -0.191194             -0.168363   \n",
       "11030                    -0.204349             -0.230895   \n",
       "\n",
       "       server_batch_counts_sum  transport_protocol  ip_protocol_version  \n",
       "0                    -0.221811           -0.270214                  NaN  \n",
       "1                    -0.228555           -0.270214                  NaN  \n",
       "8                    -0.215067           -0.270214                  NaN  \n",
       "10                   -0.228555           -0.270214                  NaN  \n",
       "19                   -0.194833            3.699982                  NaN  \n",
       "...                        ...                 ...                  ...  \n",
       "11026                -0.201578           -0.270214                  NaN  \n",
       "11027                -0.221811           -0.270214                  NaN  \n",
       "11028                -0.215067           -0.270214                  NaN  \n",
       "11029                -0.127389           -0.270214                  NaN  \n",
       "11030                -0.221811           -0.270214                  NaN  \n",
       "\n",
       "[4687 rows x 25 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
